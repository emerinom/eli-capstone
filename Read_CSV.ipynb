{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import os\n",
    "import configparser\n",
    "import  pyspark.sql.functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder\\\n",
    "                     .config(\"spark.jars.packages\",\"org.apache.hadoop:hadoop-aws:2.7.0\")\\\n",
    "                     .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.csv(\"Co2.csv\",sep=\",\", inferSchema=True, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Day: integer (nullable = true)\n",
      " |-- Samples: integer (nullable = true)\n",
      " |-- Mean: double (nullable = true)\n",
      " |-- Median: double (nullable = true)\n",
      " |-- Stdev: double (nullable = true)\n",
      "\n",
      "+--------+-------+------------------+------------------+------------------+\n",
      "|     Day|Samples|              Mean|            Median|             Stdev|\n",
      "+--------+-------+------------------+------------------+------------------+\n",
      "|20020901|     41| 372.8049011230469| 372.7300109863281|3.2957961559295654|\n",
      "|20020901|     56| 370.8941345214844|370.68951416015625|2.3466877937316895|\n",
      "|20020902|     39| 373.6762390136719| 373.3349914550781| 2.734053611755371|\n",
      "|20020903|     30|373.72796630859375|373.25299072265625| 2.933104991912842|\n",
      "|20020903|     43| 371.3970031738281| 371.3280029296875|3.5585501194000244|\n",
      "+--------+-------+------------------+------------------+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Day: integer (nullable = true)\n",
      " |-- Samples: integer (nullable = true)\n",
      " |-- Mean: double (nullable = true)\n",
      " |-- Median: double (nullable = true)\n",
      " |-- Stdev: double (nullable = true)\n",
      " |-- NewDay: timestamp (nullable = true)\n",
      "\n",
      "+--------+-------+------------------+------------------+------------------+-------------------+\n",
      "|     Day|Samples|              Mean|            Median|             Stdev|             NewDay|\n",
      "+--------+-------+------------------+------------------+------------------+-------------------+\n",
      "|20020901|     41| 372.8049011230469| 372.7300109863281|3.2957961559295654|1970-08-20 17:21:41|\n",
      "|20020901|     56| 370.8941345214844|370.68951416015625|2.3466877937316895|1970-08-20 17:21:41|\n",
      "|20020902|     39| 373.6762390136719| 373.3349914550781| 2.734053611755371|1970-08-20 17:21:42|\n",
      "|20020903|     30|373.72796630859375|373.25299072265625| 2.933104991912842|1970-08-20 17:21:43|\n",
      "|20020903|     43| 371.3970031738281| 371.3280029296875|3.5585501194000244|1970-08-20 17:21:43|\n",
      "+--------+-------+------------------+------------------+------------------+-------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfDate = df.withColumn(\"NewDay\", F.to_timestamp(\"Day\"))\n",
    "dfDate.printSchema()\n",
    "dfDate.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfDate = df.withColumn(\"month\", F.month(\"payment_date\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType as R, StructField as Fld, StringType as Str, DoubleType as Dbl,  IntegerType as Int, DateType as Date\n",
    "Co2Schema = R([\n",
    "    Fld(\"Day\",Date()),\n",
    "    Fld(\"Samples\",Int()),\n",
    "    Fld(\"Mean\",Dbl()),\n",
    "    Fld(\"Median\",Dbl()),\n",
    "    Fld(\"Stdev\",Dbl()),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfCo2WithSchema = spark.read.csv(\"Co2.csv\", sep=\",\", schema=Co2Schema, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Day: date (nullable = true)\n",
      " |-- Samples: integer (nullable = true)\n",
      " |-- Mean: double (nullable = true)\n",
      " |-- Median: double (nullable = true)\n",
      " |-- Stdev: double (nullable = true)\n",
      "\n",
      "+----------+-------+------------------+------------------+------------------+\n",
      "|       Day|Samples|              Mean|            Median|             Stdev|\n",
      "+----------+-------+------------------+------------------+------------------+\n",
      "|2002-09-01|     41| 372.8049011230469| 372.7300109863281|3.2957961559295654|\n",
      "|2002-09-01|     56| 370.8941345214844|370.68951416015625|2.3466877937316895|\n",
      "|2002-09-02|     39| 373.6762390136719| 373.3349914550781| 2.734053611755371|\n",
      "|2002-09-03|     30|373.72796630859375|373.25299072265625| 2.933104991912842|\n",
      "|2002-09-03|     43| 371.3970031738281| 371.3280029296875|3.5585501194000244|\n",
      "+----------+-------+------------------+------------------+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfCo2WithSchema.printSchema()\n",
    "dfCo2WithSchema.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------------+\n",
      "|       Day|           newMean|\n",
      "+----------+------------------+\n",
      "|2002-12-06| 371.7021026611328|\n",
      "|2002-12-25|   373.50634765625|\n",
      "|2005-01-16|378.61126708984375|\n",
      "|2005-06-06| 379.7440185546875|\n",
      "|2006-05-17|379.89076741536456|\n",
      "+----------+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = dfCo2WithSchema.groupby('Day').agg(F.mean('Mean').alias('newMean'))\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+------------------+\n",
      "|year|month|              Mean|\n",
      "+----+-----+------------------+\n",
      "|2005|    5|378.88365695901115|\n",
      "|2004|    6| 376.0786543048796|\n",
      "|2005|   10|379.09751073864925|\n",
      "|2003|    2|374.76837800678453|\n",
      "|2004|    8| 376.0771308898926|\n",
      "|2002|   12| 373.1512968275282|\n",
      "|2006|   12| 380.9201279628424|\n",
      "|2007|    1|381.72070187085296|\n",
      "|2003|   10|375.96485900878906|\n",
      "|2004|   10| 376.8089548746745|\n",
      "|2006|    7|380.28902882543105|\n",
      "|2005|    6| 378.8884950402665|\n",
      "|2003|    3| 374.7901023415958|\n",
      "|2002|   11| 372.5125354003906|\n",
      "|2006|    6|380.35894947320645|\n",
      "|2003|    9| 375.7058285444211|\n",
      "|2003|   12| 374.9952166521991|\n",
      "|2003|    1|  373.959287076383|\n",
      "|2005|   11| 379.0090730794271|\n",
      "|2004|    7| 376.2038090693486|\n",
      "+----+-----+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfCo2WithSchema.createOrReplaceTempView(\"Co2\")\n",
    "spark.sql(\"\"\"\n",
    "    SELECT year(Day) as year, month(Day) as month, Avg(Mean) as Mean\n",
    "    FROM Co2\n",
    "    GROUP by year, month\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_data = \"parquet\"\n",
    "# extract columns\n",
    "Co2_table = df\n",
    "# write table to parquet files\n",
    "Co2_table.write \\\n",
    ".mode(\"overwrite\") \\\n",
    ".format(\"parquet\") \\\n",
    ".save(\"Co2.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1568"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read in song data to use for Co2 table\n",
    "Co2_df = spark.read.parquet(\"Co2.parquet\")\n",
    "Co2_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------------+\n",
      "|       Day|           newMean|\n",
      "+----------+------------------+\n",
      "|2002-09-01| 371.8495178222656|\n",
      "|2002-09-02| 373.6762390136719|\n",
      "|2002-09-03|372.56248474121094|\n",
      "|2002-09-04|372.17618560791016|\n",
      "|2002-09-05| 372.4253387451172|\n",
      "|2002-09-06|371.29062906901044|\n",
      "|2002-09-07| 371.8840637207031|\n",
      "|2002-09-08| 371.2864227294922|\n",
      "|2002-09-09|   371.52001953125|\n",
      "|2002-09-10| 370.8514404296875|\n",
      "|2002-09-11|373.46527099609375|\n",
      "|2002-09-13|371.64884185791016|\n",
      "|2002-09-14|   372.70751953125|\n",
      "|2002-09-15| 371.2467447916667|\n",
      "|2002-09-16| 372.9146423339844|\n",
      "|2002-09-17| 372.0543518066406|\n",
      "|2002-09-18|372.36448669433594|\n",
      "|2002-09-19|371.03106689453125|\n",
      "|2002-09-20|372.19069417317706|\n",
      "|2002-09-21|371.51136779785156|\n",
      "+----------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Write code here\n",
    "Co2_df.createOrReplaceTempView(\"Co2\")\n",
    "spark.sql(\"\"\"\n",
    "    SELECT *\n",
    "    FROM Co2\n",
    "    GROUP Order by Day ASC\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
